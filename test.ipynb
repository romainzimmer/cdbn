{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of content: <a class=\"anchor\" id=\"top\" name=\"top\"></a>\n",
    "- [Introduction](#intro)\n",
    "- [Data preparation](#data-preparation)\n",
    "- [Model selection, training and evaluation](#model-training)\n",
    "- [Next steps](#next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Introduction <a class=\"anchor\" id=\"intro\" name=\"intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-> table of content](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to predict the daily sales based on the available information (date, product, ...) for different discounts.\n",
    "\n",
    "I decided to train a model to predict the sales for any possible discount. The discount is thus considered as an input feature for the model. If we only wish to predict the sales for a small set of discount values (e.g -10%, -20%, -30%, ...), another approach could be to train a different model for each discount value but that would require more resources.\n",
    "\n",
    "I also assume that all the sales information is present in the data, i.e. the sales are reported even if they are null. And, if no sales are reported for a given product at a given date, it means that none of them were for sale. Otherwise, we should add the missing data.\n",
    "\n",
    "In the following, I first prepare the data using [pandas](https://pandas.pydata.org/), then use the [scikit-learn](https://scikit-learn.org/stable/) library to try different set of parameters to train an [XGBoost](https://xgboost.readthedocs.io/en/stable/) model. I chose XGBoost because it is a versatile model that performs very well on many tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Data preparation <a class=\"anchor\" id=\"data-preparation\" name=\"data-preparation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-> table of content](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found some data with a negative number of sales, some undiscounted sales with a price of 0 and some cases where the discounted price was larger than the undiscounted price. I decided to discard them as they were very few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data count: 138882\n",
      "Cleaned data count: 138724\n"
     ]
    }
   ],
   "source": [
    "DATA_FILE = \"data/test_technique ML eng - data.csv\"\n",
    "\n",
    "df = pd.read_csv(DATA_FILE, sep=';', dtype={'ean': 'str'}, parse_dates=['date'], date_parser=lambda x: datetime.strptime(x, \"%Y-%m-%d\"))\n",
    "print(\"Raw data count:\", len(df))\n",
    "\n",
    "# remove data with negative sales\n",
    "df.drop(df[(df['nb_vente'] < 0) | (df['nb_vente_kcf'] < 0)].index, inplace=True)\n",
    "\n",
    "# remove data where pv=0\n",
    "df.drop(df[df['pv'] == 0 ].index, inplace=True)\n",
    "\n",
    "# remove data where pv_kcf >= pv\n",
    "df.drop(df[df['pv'] <= df['pv_kcf'] ].index, inplace=True)\n",
    "\n",
    "print(\"Cleaned data count:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting discount data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row can contain some data about discounted and undiscounted sales. In this part, I split the rows to get a single price and sales count per row. I also added the discount percentage as a new feature as the discount could have a psychological effect on the customers and thus should be taken into account when trying to predict the sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_COLS = ['date', 'ean', 'qt_casse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_undiscounted_data(df):\n",
    "\n",
    "    undiscounted_df = df.loc[:, METADATA_COLS + ['pv', 'nb_vente']]\n",
    "    undiscounted_df.rename(columns={'pv': 'price', 'nb_vente': 'sales_count'}, inplace=True)\n",
    "    undiscounted_df['disc_percentage'] = 0\n",
    "\n",
    "    return undiscounted_df\n",
    "\n",
    "def extract_discounted_data(df):\n",
    "\n",
    "    masked_df = df[df['pv_kcf'] > 0]\n",
    "    discounted_df = masked_df.loc[:, METADATA_COLS + ['pv_kcf', 'nb_vente_kcf']]\n",
    "    discounted_df.rename(columns={'pv_kcf': 'price', 'nb_vente_kcf': 'sales_count'}, inplace=True)\n",
    "    discounted_df['disc_percentage'] = masked_df['pv_kcf'] / masked_df['pv']\n",
    "\n",
    "    return discounted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undiscounted data count: 138724\n",
      "Discounted data count: 9405\n",
      "All discounts data count: 148129\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ean</th>\n",
       "      <th>qt_casse</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_count</th>\n",
       "      <th>disc_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>40846.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.49</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>3760253432320.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>3222473152865.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>3222473233687.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.95</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>3760253430623.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date              ean  qt_casse  price  sales_count  disc_percentage\n",
       "0 2022-04-01          40846.0       0.0   1.49          2.0              0.0\n",
       "1 2022-04-01  3760253432320.0       0.0   5.65          1.0              0.0\n",
       "2 2022-04-01  3222473152865.0       0.0   5.20          2.0              0.0\n",
       "3 2022-04-01  3222473233687.0       0.0   3.95          6.0              0.0\n",
       "4 2022-04-01  3760253430623.0       0.0   8.50          1.0              0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "undiscounted_df = extract_undiscounted_data(df)\n",
    "print(\"Undiscounted data count:\", len(undiscounted_df))\n",
    "\n",
    "discounted_df = extract_discounted_data(df)\n",
    "print(\"Discounted data count:\", len(discounted_df))\n",
    "\n",
    "all_discounts_df = pd.concat([undiscounted_df, discounted_df])\n",
    "print(\"All discounts data count:\", len(all_discounts_df))\n",
    "\n",
    "all_discounts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the amount of data about discounted sales is much lower than the amount of data about undiscounted sales. This could affect the model ability to predict discounted sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generating date features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data span 5 months of 2022. Thus, we don't have enough information to change the prediction based on the year or month. I only keep the day of the week (monday, tuesday, ...) and weither it's the beginning, middle or end of the month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_date_features(df):\n",
    "\n",
    "    def get_month_part(x):\n",
    "        if x.day <= 10:\n",
    "            return 0\n",
    "        elif x.day <= 20:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "\n",
    "    df['month_part'] = df['date'].map(lambda x: get_month_part(x))\n",
    "    df['weekday'] = df['date'].map(lambda x: x.weekday())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_date_features(all_discounts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ean</th>\n",
       "      <th>qt_casse</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_count</th>\n",
       "      <th>disc_percentage</th>\n",
       "      <th>month_part</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>40846.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.49</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>3760253432320.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>3222473152865.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>3222473233687.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.95</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>3760253430623.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date              ean  qt_casse  price  sales_count  disc_percentage  \\\n",
       "0 2022-04-01          40846.0       0.0   1.49          2.0              0.0   \n",
       "1 2022-04-01  3760253432320.0       0.0   5.65          1.0              0.0   \n",
       "2 2022-04-01  3222473152865.0       0.0   5.20          2.0              0.0   \n",
       "3 2022-04-01  3222473233687.0       0.0   3.95          6.0              0.0   \n",
       "4 2022-04-01  3760253430623.0       0.0   8.50          1.0              0.0   \n",
       "\n",
       "   month_part  weekday  \n",
       "0           0        4  \n",
       "1           0        4  \n",
       "2           0        4  \n",
       "3           0        4  \n",
       "4           0        4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_discounts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating aggregated features based on product identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are around 2.5k different products in the data. We cannot give their ids directly to the model and a simple [one hot encoding](https://en.wikipedia.org/wiki/One-hot) would require us to add 2.5k new columns. A good approach is to use this information to generate aggregated features instead. \n",
    "\n",
    "Thus, for each product I compute:\n",
    "- the average undiscounted price\n",
    "- the average trashed quantity\n",
    "- the average number of days between undiscounted sales\n",
    "\n",
    "I decided not to take discounted sales information into account for aggregated features as they might add a lot of variability.\n",
    "\n",
    "Also, a critical point is to avoid a so called \"[information leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning))\" by using information of future sales to predict the current ones as this information would not be available in real case scenarios. Thus, for each record, we can only aggregate information from the previous records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_aggregated_features(df):\n",
    "\n",
    "    df.loc[:, ('agg_price', 'agg_qt_casse', 'agg_sales_count')] = np.nan\n",
    "\n",
    "    def add_agg_features(group):\n",
    "        # get undiscounted rows\n",
    "        group_undisc = group[group['disc_percentage']==0].sort_values(by='date', ascending=True)\n",
    "\n",
    "        for x in group.itertuples():\n",
    "\n",
    "            # get previous sales\n",
    "            prev_undisc = group_undisc[group_undisc['date'] < x.date]\n",
    "\n",
    "            agg_days_bt_sales = prev_undisc[prev_undisc['sales_count'] > 0]['date'].diff().map(lambda x: x.days).mean()\n",
    "            agg_price = prev_undisc['price'].mean()\n",
    "            agg_qt_casse = prev_undisc['qt_casse'].mean()\n",
    "            agg_sales_count = prev_undisc['sales_count'].mean()\n",
    "\n",
    "            df.loc[x.Index, 'agg_days_bt_sales'] = agg_days_bt_sales\n",
    "            df.loc[x.Index, 'agg_price'] = agg_price\n",
    "            df.loc[x.Index, 'agg_qt_casse'] = agg_qt_casse\n",
    "            df.loc[x.Index, 'agg_sales_count'] = agg_sales_count\n",
    "    \n",
    "    df.groupby('ean').apply(add_agg_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ean</th>\n",
       "      <th>qt_casse</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_count</th>\n",
       "      <th>disc_percentage</th>\n",
       "      <th>month_part</th>\n",
       "      <th>weekday</th>\n",
       "      <th>agg_price</th>\n",
       "      <th>agg_qt_casse</th>\n",
       "      <th>agg_sales_count</th>\n",
       "      <th>agg_days_bt_sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138773</th>\n",
       "      <td>2022-08-31</td>\n",
       "      <td>3222476292353.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.664000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.203806</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>1.733333</td>\n",
       "      <td>1.985915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138796</th>\n",
       "      <td>2022-08-31</td>\n",
       "      <td>3222474499242.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.789286</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.864117</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>2.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138831</th>\n",
       "      <td>2022-08-31</td>\n",
       "      <td>3180950007292.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.660194</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.739446</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>1.802469</td>\n",
       "      <td>1.911392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138869</th>\n",
       "      <td>2022-08-31</td>\n",
       "      <td>3222477010390.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.658228</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.959337</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>2.933333</td>\n",
       "      <td>2.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138879</th>\n",
       "      <td>2022-08-31</td>\n",
       "      <td>3222474698461.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.642436</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4.760699</td>\n",
       "      <td>0.224138</td>\n",
       "      <td>2.206897</td>\n",
       "      <td>2.830189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date              ean  qt_casse  price  sales_count  \\\n",
       "138773 2022-08-31  3222476292353.0       0.0   0.83          3.0   \n",
       "138796 2022-08-31  3222474499242.0       0.0   2.21          1.0   \n",
       "138831 2022-08-31  3180950007292.0       0.0   2.04          1.0   \n",
       "138869 2022-08-31  3222477010390.0       0.0   2.08          1.0   \n",
       "138879 2022-08-31  3222474698461.0       0.0   3.27          1.0   \n",
       "\n",
       "        disc_percentage  month_part  weekday  agg_price  agg_qt_casse  \\\n",
       "138773         0.664000           2        2   1.203806      0.160000   \n",
       "138796         0.789286           2        2   2.864117      0.089286   \n",
       "138831         0.660194           2        2   2.739446      0.049383   \n",
       "138869         0.658228           2        2   2.959337      0.293333   \n",
       "138879         0.642436           2        2   4.760699      0.224138   \n",
       "\n",
       "        agg_sales_count  agg_days_bt_sales  \n",
       "138773         1.733333           1.985915  \n",
       "138796         1.625000           2.882353  \n",
       "138831         1.802469           1.911392  \n",
       "138869         2.933333           2.083333  \n",
       "138879         2.206897           2.830189  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this can take a few minutes, do it once then use the saved result directly.\n",
    "generate_aggregated_features(all_discounts_df)\n",
    "\n",
    "# saving the final dataframe\n",
    "DF_FILE = 'data/all_discounts_df.csv'\n",
    "all_discounts_df.to_csv(DF_FILE, index=False)\n",
    "\n",
    "all_discounts_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Model selection, training and evaluation <a class=\"anchor\" id=\"model-training\" name=\"model-training\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-> table of content](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from collections import OrderedDict\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating the features and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we keep the usefull columns for our model and generate dummy columns using one hot encoding for month parts and the days of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_COLS = ['price', 'disc_percentage', 'month_part', 'weekday', 'agg_price', 'agg_qt_casse', 'agg_sales_count', \"agg_days_bt_sales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>disc_percentage</th>\n",
       "      <th>agg_price</th>\n",
       "      <th>agg_qt_casse</th>\n",
       "      <th>agg_sales_count</th>\n",
       "      <th>agg_days_bt_sales</th>\n",
       "      <th>month_parts_0</th>\n",
       "      <th>month_parts_1</th>\n",
       "      <th>month_parts_2</th>\n",
       "      <th>weekday_0</th>\n",
       "      <th>weekday_1</th>\n",
       "      <th>weekday_2</th>\n",
       "      <th>weekday_3</th>\n",
       "      <th>weekday_4</th>\n",
       "      <th>weekday_5</th>\n",
       "      <th>weekday_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138215</th>\n",
       "      <td>3.55</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.396154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.692308</td>\n",
       "      <td>3.894737</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138216</th>\n",
       "      <td>5.85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.991727</td>\n",
       "      <td>0.104478</td>\n",
       "      <td>1.895522</td>\n",
       "      <td>2.426230</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138217</th>\n",
       "      <td>3.99</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.227279</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1.795455</td>\n",
       "      <td>3.609756</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138147</th>\n",
       "      <td>8.70</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.260870</td>\n",
       "      <td>4.727273</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148128</th>\n",
       "      <td>3.27</td>\n",
       "      <td>0.642436</td>\n",
       "      <td>4.760699</td>\n",
       "      <td>0.224138</td>\n",
       "      <td>2.206897</td>\n",
       "      <td>2.830189</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        price  disc_percentage  agg_price  agg_qt_casse  agg_sales_count  \\\n",
       "138215   3.55         0.000000   3.396154      0.000000         1.692308   \n",
       "138216   5.85         0.000000   4.991727      0.104478         1.895522   \n",
       "138217   3.99         0.000000   3.227279      0.363636         1.795455   \n",
       "138147   8.70         0.000000   8.100000      0.000000         2.260870   \n",
       "148128   3.27         0.642436   4.760699      0.224138         2.206897   \n",
       "\n",
       "        agg_days_bt_sales  month_parts_0  month_parts_1  month_parts_2  \\\n",
       "138215           3.894737              0              0              1   \n",
       "138216           2.426230              0              0              1   \n",
       "138217           3.609756              0              0              1   \n",
       "138147           4.727273              0              0              1   \n",
       "148128           2.830189              0              0              1   \n",
       "\n",
       "        weekday_0  weekday_1  weekday_2  weekday_3  weekday_4  weekday_5  \\\n",
       "138215          0          0          1          0          0          0   \n",
       "138216          0          0          1          0          0          0   \n",
       "138217          0          0          1          0          0          0   \n",
       "138147          0          0          1          0          0          0   \n",
       "148128          0          0          1          0          0          0   \n",
       "\n",
       "        weekday_6  \n",
       "138215          0  \n",
       "138216          0  \n",
       "138217          0  \n",
       "138147          0  \n",
       "148128          0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_FILE = 'data/all_discounts_df.csv'\n",
    "all_discounts_df = pd.read_csv(DF_FILE).sort_values(by='date') # ensure sorted by date\n",
    "X = all_discounts_df.loc[:, FEATURES_COLS]\n",
    "X = pd.get_dummies(X, prefix='month_parts', columns=['month_part'])\n",
    "X = pd.get_dummies(X, prefix='weekday', columns=['weekday'])\n",
    "y = all_discounts_df['sales_count']\n",
    "\n",
    "X.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to select good hyperparameters for our models (meta parameters that impact the training and performance of the model). To do that, we usually train the same model on the same data with different set of parameters. Vairous approaches exist, in this case we use a randomized search in which we pick hyperparameters randomly among a set of possible values. It is a simple approach that allows us to test different settings easily.\n",
    "\n",
    "To evaluate the model, we cannot use a standard [cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)). Indeed, here too we should carefully avoid \"information leakage\". Thus, we instead use scikit-learn [TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) to ensure that we only test the model on future sales (instead of potentially training on future sales to predict the current ones).\n",
    "\n",
    "The metric used to train the model is a standard squared error. I chose this metric rather than the absolute error as I assumed that 1 big sales predictions mistakes is worse than a few small ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV 1/3; 1/10] START colsample_bytree=0.8, gamma=10, learning_rate=0.1, max_depth=7, min_child_weight=10, n_estimators=200, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=10, subsample=0.6\n",
      "[CV 1/3; 1/10] END colsample_bytree=0.8, gamma=10, learning_rate=0.1, max_depth=7, min_child_weight=10, n_estimators=200, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=10, subsample=0.6;, score=-2.492 total time=   6.2s\n",
      "[CV 2/3; 1/10] START colsample_bytree=0.8, gamma=10, learning_rate=0.1, max_depth=7, min_child_weight=10, n_estimators=200, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=10, subsample=0.6\n",
      "[CV 2/3; 1/10] END colsample_bytree=0.8, gamma=10, learning_rate=0.1, max_depth=7, min_child_weight=10, n_estimators=200, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=10, subsample=0.6;, score=-2.690 total time=  16.2s\n",
      "[CV 3/3; 1/10] START colsample_bytree=0.8, gamma=10, learning_rate=0.1, max_depth=7, min_child_weight=10, n_estimators=200, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=10, subsample=0.6\n",
      "[CV 3/3; 1/10] END colsample_bytree=0.8, gamma=10, learning_rate=0.1, max_depth=7, min_child_weight=10, n_estimators=200, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=10, subsample=0.6;, score=-2.604 total time=  19.3s\n",
      "[CV 1/3; 2/10] START colsample_bytree=0.4, gamma=0, learning_rate=0.2, max_depth=5, min_child_weight=1, n_estimators=500, objective=reg:squarederror, reg_alpha=0.1, reg_lambda=0.1, subsample=0.6\n",
      "[CV 1/3; 2/10] END colsample_bytree=0.4, gamma=0, learning_rate=0.2, max_depth=5, min_child_weight=1, n_estimators=500, objective=reg:squarederror, reg_alpha=0.1, reg_lambda=0.1, subsample=0.6;, score=-2.678 total time=   7.5s\n",
      "[CV 2/3; 2/10] START colsample_bytree=0.4, gamma=0, learning_rate=0.2, max_depth=5, min_child_weight=1, n_estimators=500, objective=reg:squarederror, reg_alpha=0.1, reg_lambda=0.1, subsample=0.6\n",
      "[CV 2/3; 2/10] END colsample_bytree=0.4, gamma=0, learning_rate=0.2, max_depth=5, min_child_weight=1, n_estimators=500, objective=reg:squarederror, reg_alpha=0.1, reg_lambda=0.1, subsample=0.6;, score=-2.783 total time=  15.7s\n",
      "[CV 3/3; 2/10] START colsample_bytree=0.4, gamma=0, learning_rate=0.2, max_depth=5, min_child_weight=1, n_estimators=500, objective=reg:squarederror, reg_alpha=0.1, reg_lambda=0.1, subsample=0.6\n",
      "[CV 3/3; 2/10] END colsample_bytree=0.4, gamma=0, learning_rate=0.2, max_depth=5, min_child_weight=1, n_estimators=500, objective=reg:squarederror, reg_alpha=0.1, reg_lambda=0.1, subsample=0.6;, score=-2.669 total time=  22.5s\n",
      "[CV 1/3; 3/10] START colsample_bytree=0.4, gamma=10, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=0.01, subsample=0.95\n",
      "[CV 1/3; 3/10] END colsample_bytree=0.4, gamma=10, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=0.01, subsample=0.95;, score=-2.505 total time=   1.9s\n",
      "[CV 2/3; 3/10] START colsample_bytree=0.4, gamma=10, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=0.01, subsample=0.95\n",
      "[CV 2/3; 3/10] END colsample_bytree=0.4, gamma=10, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=0.01, subsample=0.95;, score=-2.707 total time=   3.5s\n",
      "[CV 3/3; 3/10] START colsample_bytree=0.4, gamma=10, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=0.01, subsample=0.95\n",
      "[CV 3/3; 3/10] END colsample_bytree=0.4, gamma=10, learning_rate=0.1, max_depth=3, min_child_weight=5, n_estimators=200, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=0.01, subsample=0.95;, score=-2.600 total time=   5.7s\n",
      "[CV 1/3; 4/10] START colsample_bytree=0.4, gamma=10, learning_rate=0.2, max_depth=5, min_child_weight=10, n_estimators=500, objective=reg:squarederror, reg_alpha=0.1, reg_lambda=0.1, subsample=0.6\n",
      "[CV 1/3; 4/10] END colsample_bytree=0.4, gamma=10, learning_rate=0.2, max_depth=5, min_child_weight=10, n_estimators=500, objective=reg:squarederror, reg_alpha=0.1, reg_lambda=0.1, subsample=0.6;, score=-2.594 total time=   7.4s\n",
      "[CV 2/3; 4/10] START colsample_bytree=0.4, gamma=10, learning_rate=0.2, max_depth=5, min_child_weight=10, n_estimators=500, objective=reg:squarederror, reg_alpha=0.1, reg_lambda=0.1, subsample=0.6\n",
      "[CV 2/3; 4/10] END colsample_bytree=0.4, gamma=10, learning_rate=0.2, max_depth=5, min_child_weight=10, n_estimators=500, objective=reg:squarederror, reg_alpha=0.1, reg_lambda=0.1, subsample=0.6;, score=-2.750 total time=  14.7s\n",
      "[CV 3/3; 4/10] START colsample_bytree=0.4, gamma=10, learning_rate=0.2, max_depth=5, min_child_weight=10, n_estimators=500, objective=reg:squarederror, reg_alpha=0.1, reg_lambda=0.1, subsample=0.6\n",
      "[CV 3/3; 4/10] END colsample_bytree=0.4, gamma=10, learning_rate=0.2, max_depth=5, min_child_weight=10, n_estimators=500, objective=reg:squarederror, reg_alpha=0.1, reg_lambda=0.1, subsample=0.6;, score=-2.634 total time=  22.8s\n",
      "[CV 1/3; 5/10] START colsample_bytree=0.6, gamma=0, learning_rate=0.01, max_depth=6, min_child_weight=1, n_estimators=500, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=1, subsample=0.6\n",
      "[CV 1/3; 5/10] END colsample_bytree=0.6, gamma=0, learning_rate=0.01, max_depth=6, min_child_weight=1, n_estimators=500, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=1, subsample=0.6;, score=-2.507 total time=  12.0s\n",
      "[CV 2/3; 5/10] START colsample_bytree=0.6, gamma=0, learning_rate=0.01, max_depth=6, min_child_weight=1, n_estimators=500, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=1, subsample=0.6\n",
      "[CV 2/3; 5/10] END colsample_bytree=0.6, gamma=0, learning_rate=0.01, max_depth=6, min_child_weight=1, n_estimators=500, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=1, subsample=0.6;, score=-2.693 total time=  26.5s\n",
      "[CV 3/3; 5/10] START colsample_bytree=0.6, gamma=0, learning_rate=0.01, max_depth=6, min_child_weight=1, n_estimators=500, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=1, subsample=0.6\n",
      "[CV 3/3; 5/10] END colsample_bytree=0.6, gamma=0, learning_rate=0.01, max_depth=6, min_child_weight=1, n_estimators=500, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=1, subsample=0.6;, score=-2.586 total time=  44.3s\n",
      "[CV 1/3; 6/10] START colsample_bytree=0.4, gamma=0.1, learning_rate=0.1, max_depth=7, min_child_weight=10, n_estimators=500, objective=reg:squarederror, reg_alpha=10, reg_lambda=10, subsample=0.6\n",
      "[CV 1/3; 6/10] END colsample_bytree=0.4, gamma=0.1, learning_rate=0.1, max_depth=7, min_child_weight=10, n_estimators=500, objective=reg:squarederror, reg_alpha=10, reg_lambda=10, subsample=0.6;, score=-2.533 total time=  10.2s\n",
      "[CV 2/3; 6/10] START colsample_bytree=0.4, gamma=0.1, learning_rate=0.1, max_depth=7, min_child_weight=10, n_estimators=500, objective=reg:squarederror, reg_alpha=10, reg_lambda=10, subsample=0.6\n",
      "[CV 2/3; 6/10] END colsample_bytree=0.4, gamma=0.1, learning_rate=0.1, max_depth=7, min_child_weight=10, n_estimators=500, objective=reg:squarederror, reg_alpha=10, reg_lambda=10, subsample=0.6;, score=-2.712 total time=  22.8s\n",
      "[CV 3/3; 6/10] START colsample_bytree=0.4, gamma=0.1, learning_rate=0.1, max_depth=7, min_child_weight=10, n_estimators=500, objective=reg:squarederror, reg_alpha=10, reg_lambda=10, subsample=0.6\n",
      "[CV 3/3; 6/10] END colsample_bytree=0.4, gamma=0.1, learning_rate=0.1, max_depth=7, min_child_weight=10, n_estimators=500, objective=reg:squarederror, reg_alpha=10, reg_lambda=10, subsample=0.6;, score=-2.609 total time=  32.0s\n",
      "[CV 1/3; 7/10] START colsample_bytree=0.4, gamma=10, learning_rate=0.1, max_depth=8, min_child_weight=5, n_estimators=200, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=10, subsample=0.95\n",
      "[CV 1/3; 7/10] END colsample_bytree=0.4, gamma=10, learning_rate=0.1, max_depth=8, min_child_weight=5, n_estimators=200, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=10, subsample=0.95;, score=-2.510 total time=   4.9s\n",
      "[CV 2/3; 7/10] START colsample_bytree=0.4, gamma=10, learning_rate=0.1, max_depth=8, min_child_weight=5, n_estimators=200, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=10, subsample=0.95\n",
      "[CV 2/3; 7/10] END colsample_bytree=0.4, gamma=10, learning_rate=0.1, max_depth=8, min_child_weight=5, n_estimators=200, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=10, subsample=0.95;, score=-2.703 total time=   8.7s\n",
      "[CV 3/3; 7/10] START colsample_bytree=0.4, gamma=10, learning_rate=0.1, max_depth=8, min_child_weight=5, n_estimators=200, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=10, subsample=0.95\n",
      "[CV 3/3; 7/10] END colsample_bytree=0.4, gamma=10, learning_rate=0.1, max_depth=8, min_child_weight=5, n_estimators=200, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=10, subsample=0.95;, score=-2.600 total time=  14.0s\n",
      "[CV 1/3; 8/10] START colsample_bytree=0.6, gamma=0.1, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_alpha=1, reg_lambda=0.1, subsample=0.95\n",
      "[CV 1/3; 8/10] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_alpha=1, reg_lambda=0.1, subsample=0.95;, score=-2.513 total time=   9.8s\n",
      "[CV 2/3; 8/10] START colsample_bytree=0.6, gamma=0.1, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_alpha=1, reg_lambda=0.1, subsample=0.95\n",
      "[CV 2/3; 8/10] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_alpha=1, reg_lambda=0.1, subsample=0.95;, score=-2.699 total time=  20.1s\n",
      "[CV 3/3; 8/10] START colsample_bytree=0.6, gamma=0.1, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_alpha=1, reg_lambda=0.1, subsample=0.95\n",
      "[CV 3/3; 8/10] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.01, max_depth=5, min_child_weight=5, n_estimators=500, objective=reg:squarederror, reg_alpha=1, reg_lambda=0.1, subsample=0.95;, score=-2.591 total time=  32.9s\n",
      "[CV 1/3; 9/10] START colsample_bytree=0.8, gamma=10, learning_rate=0.01, max_depth=3, min_child_weight=10, n_estimators=500, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=0.1, subsample=0.95\n",
      "[CV 1/3; 9/10] END colsample_bytree=0.8, gamma=10, learning_rate=0.01, max_depth=3, min_child_weight=10, n_estimators=500, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=0.1, subsample=0.95;, score=-2.524 total time=   7.4s\n",
      "[CV 2/3; 9/10] START colsample_bytree=0.8, gamma=10, learning_rate=0.01, max_depth=3, min_child_weight=10, n_estimators=500, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=0.1, subsample=0.95\n",
      "[CV 2/3; 9/10] END colsample_bytree=0.8, gamma=10, learning_rate=0.01, max_depth=3, min_child_weight=10, n_estimators=500, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=0.1, subsample=0.95;, score=-2.721 total time=  15.7s\n",
      "[CV 3/3; 9/10] START colsample_bytree=0.8, gamma=10, learning_rate=0.01, max_depth=3, min_child_weight=10, n_estimators=500, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=0.1, subsample=0.95\n",
      "[CV 3/3; 9/10] END colsample_bytree=0.8, gamma=10, learning_rate=0.01, max_depth=3, min_child_weight=10, n_estimators=500, objective=reg:squarederror, reg_alpha=0.01, reg_lambda=0.1, subsample=0.95;, score=-2.604 total time=  24.2s\n",
      "[CV 1/3; 10/10] START colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=500, objective=reg:squarederror, reg_alpha=0.1, reg_lambda=0.01, subsample=0.95\n",
      "[CV 1/3; 10/10] END colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=500, objective=reg:squarederror, reg_alpha=0.1, reg_lambda=0.01, subsample=0.95;, score=-2.512 total time=  10.2s\n",
      "[CV 2/3; 10/10] START colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=500, objective=reg:squarederror, reg_alpha=0.1, reg_lambda=0.01, subsample=0.95\n",
      "[CV 2/3; 10/10] END colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=500, objective=reg:squarederror, reg_alpha=0.1, reg_lambda=0.01, subsample=0.95;, score=-2.706 total time=  21.9s\n",
      "[CV 3/3; 10/10] START colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=500, objective=reg:squarederror, reg_alpha=0.1, reg_lambda=0.01, subsample=0.95\n",
      "[CV 3/3; 10/10] END colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=4, min_child_weight=1, n_estimators=500, objective=reg:squarederror, reg_alpha=0.1, reg_lambda=0.01, subsample=0.95;, score=-2.595 total time=  31.4s\n",
      "best score\n",
      "-2.595341608959285\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params_distributions = {\n",
    "   'colsample_bytree':[0.4,0.6,0.8],\n",
    "   'gamma':[0, 0.1, 10],\n",
    "   'min_child_weight':[1, 5, 10],\n",
    "   'learning_rate':[0.01, 0.1, 0.2],\n",
    "   'max_depth': range(3,10),\n",
    "   'reg_alpha':[1e-2, 1e-1,  1, 10],\n",
    "   'reg_lambda':[1e-2, 1e-1, 1, 10],\n",
    "   'subsample':[0.6,0.95],\n",
    "   \"objective\": [\"reg:squarederror\"],\n",
    "   \"n_estimators\": [200, 300, 500]\n",
    "}\n",
    "\n",
    "                    \n",
    "xgb_model = xgb.XGBRegressor(seed=27)\n",
    "cv = TimeSeriesSplit(n_splits=3).split(X)\n",
    "\n",
    "rsearch = RandomizedSearchCV(\n",
    "   estimator = xgb_model,\n",
    "   param_distributions = params_distributions,\n",
    "   n_iter = 10,\n",
    "   verbose = 10,\n",
    "   cv = cv,\n",
    "   refit = False,\n",
    "   scoring = 'neg_root_mean_squared_error')\n",
    "\n",
    "rsearch.fit(X, y)\n",
    "\n",
    "print('best score')\n",
    "print (rsearch.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we train the model on 2/3 of the dataset and evaluate on the last one to be able to analyse a little bit more the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.8,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=10, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "             importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "             learning_rate=0.1, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=7, max_leaves=0, min_child_weight=10,\n",
       "             missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=200, n_jobs=0,\n",
       "             num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=27,\n",
       "             reg_alpha=0.01, reg_lambda=10, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.8,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=10, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "             importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "             learning_rate=0.1, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=7, max_leaves=0, min_child_weight=10,\n",
       "             missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=200, n_jobs=0,\n",
       "             num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=27,\n",
       "             reg_alpha=0.01, reg_lambda=10, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.8,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=10, gpu_id=-1, grow_policy='depthwise',\n",
       "             importance_type=None, interaction_constraints='',\n",
       "             learning_rate=0.1, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=7, max_leaves=0, min_child_weight=10,\n",
       "             missing=nan, monotone_constraints='()', n_estimators=200, n_jobs=0,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=27,\n",
       "             reg_alpha=0.01, reg_lambda=10, ...)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBRegressor(**rsearch.best_params_, seed=27)\n",
    "\n",
    "split_id = 2 * len(X) // 3\n",
    "X_train, y_train = X.iloc[:split_id], y.iloc[:split_id]\n",
    "X_test, y_test = X.iloc[split_id:], y.iloc[split_id:]\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance: (feature, usage)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('agg_sales_count', 2265.0),\n",
       "             ('price', 2170.0),\n",
       "             ('agg_price', 2122.0),\n",
       "             ('agg_days_bt_sales', 1485.0),\n",
       "             ('agg_qt_casse', 787.0),\n",
       "             ('disc_percentage', 297.0),\n",
       "             ('weekday_4', 250.0),\n",
       "             ('weekday_5', 246.0),\n",
       "             ('month_parts_0', 150.0),\n",
       "             ('month_parts_2', 148.0),\n",
       "             ('weekday_6', 146.0),\n",
       "             ('month_parts_1', 123.0),\n",
       "             ('weekday_1', 92.0),\n",
       "             ('weekday_0', 80.0),\n",
       "             ('weekday_3', 64.0),\n",
       "             ('weekday_2', 61.0)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Feature importance: (feature, usage)\")\n",
    "OrderedDict(sorted(model.get_booster().get_fscore().items(), key=lambda t: t[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the aggregated price feature has been used 11816 times by the model and the information about some days of the week only a few hundreds time. The aggregated features are thus the most important for the model. \n",
    "\n",
    "Interestingly, we can see that the model seems to base its decision on wether it's friday, saturday or sunday (weekday_4, weekday_5, weekday_6) more than the other days.\n",
    "\n",
    "Next, we will compute the root mean squared error and mean absolute error on the test set and check a few predictions by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test root mean squared error (RMSE): 2.61\n",
      "Test mean absolute error (MAE): 1.567\n",
      "\n",
      "Test RMSE (undiscounted sales only): 2.611\n",
      "Test MAE (undiscounted sales only): 1.555\n",
      "\n",
      "Test RMSE (discounted sales only): 2.596\n",
      "Test MAE (discounted sales only): 1.727\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "err = pred-y_test\n",
    "print(f\"Test root mean squared error (RMSE): {np.sqrt(err.map(lambda x: x**2).mean()):.4}\",)\n",
    "print(f\"Test mean absolute error (MAE): {err.map(abs).mean():.4}\")\n",
    "\n",
    "discounted_mask = X_test['disc_percentage'] > 0\n",
    "print(f\"\\nTest RMSE (undiscounted sales only): {np.sqrt(err[~discounted_mask].map(lambda x: x**2).mean()):.4}\")\n",
    "print(f\"Test MAE (undiscounted sales only): {err[~discounted_mask].map(abs).mean():.4}\")\n",
    "print(f\"\\nTest RMSE (discounted sales only): {np.sqrt(err[discounted_mask].map(lambda x: x**2).mean()):.4}\")\n",
    "print(f\"Test MAE (discounted sales only): {err[discounted_mask].map(abs).mean():.4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE cannot easily be interpreted but the MAE is the error that the model is doing in average for each sale prediction. \n",
    "\n",
    "We can see that the RMSE is the same on discounted and undiscounted sales but the model has a higher MAE on discounted sales. If this is a problem we could add weights to the training data so that the model focuses more on discounted sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of predictions:\n",
      "2022-08-24 7613036855693.0 prediction: 1 true value: 2.0\n",
      "2022-07-27 3222477061262.0 prediction: 4 true value: 4.0\n",
      "2022-07-13 3248832960025.0 prediction: 2 true value: 2.0\n",
      "2022-08-22 3242272348252.0 prediction: 2 true value: 1.0\n",
      "2022-08-29 3222474215545.0 prediction: 2 true value: 3.0\n",
      "2022-08-11 3302746583029.0 prediction: 2 true value: 1.0\n",
      "2022-07-23 3222477544284.0 prediction: 2 true value: 1.0\n",
      "2022-08-24 3154230800514.0 prediction: 2 true value: 1.0\n",
      "2022-08-08 3095756221011.0 prediction: 2 true value: 3.0\n",
      "2022-07-30 3180940084371.0 prediction: 7 true value: 6.0\n",
      "2022-08-06 3095756312016.0 prediction: 4 true value: 1.0\n",
      "2022-08-22 3095754137017.0 prediction: 1 true value: 1.0\n",
      "2022-08-30 3436590082268.0 prediction: 3 true value: 2.0\n",
      "2022-08-18 3222472606253.0 prediction: 4 true value: 3.0\n",
      "2022-07-09 7613034423146.0 prediction: 3 true value: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample of predictions:\")\n",
    "for _ in range(15):\n",
    "    i = rd.randint(0, len(X_test))\n",
    "    ind = X_test.index[i]\n",
    "    rpred = round(pred[i])\n",
    "    true = y_test.iloc[i]\n",
    "    print(all_discounts_df.loc[ind, 'date'], all_discounts_df.loc[ind, 'ean'],  f\"prediction: {rpred} true value: {true}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV - Next steps <a class=\"anchor\" id=\"next-steps\" name=\"next-steps\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[-> table of content](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a model can easily be deployed as an HTTP API (e.g. using [FastAPI](https://fastapi.tiangolo.com/)) with a `/predict` POST endpoint to predict daily sales one by one or by batches.\n",
    "\n",
    "The main issue is the footprint of the model as ML models can become quite big very fast. We should find the right tradeoff between having a good accuracy, a good latency / throughput, and not consuming to many resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the model, one could:\n",
    "- try to find better features and remove the ones that are not usefull\n",
    "- try more hyperparameters for the model\n",
    "- try different models than XGBoost\n",
    "- try to frame the problem in a different way e.g. predicting how much time it takes to sell a product with a given discount rather than predicting the daily sales.\n",
    "- try to add sample weights on the data to minimize imbalances\n",
    "- try to gather more data (quantity and features, e.g. stores location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to test whether our model improves or not is to compare the prediction of the model based on real world data. If we see a drop in the performance, we can simply retrain the model.\n",
    "\n",
    "However, we would be in a typical \"Exploration-Exploitation\" dilemma. Indeed, each day we can only apply 1 discount value for a given product. Therefore, we can only get the true number of sales for a single discount value. If we always choose the best discount (exploitation), the model could never learn (or forget) how to predict the sales in other scenarios and the predictions would likely not be optimal.\n",
    "\n",
    "If we take suboptimal decisions from time to time to gather data (exploration), we would improve the ability of the model to handle various situations but we would pay the price of the suboptimal discounts. Thus, a good tradeoff has to be found.\n",
    "\n",
    "Additionally, we could monitor other statistics of the data (e.g. average sales, prices etc.) to detect data distribution [drifts](https://en.wikipedia.org/wiki/Drift_(data_science)), i.e. when the real world data becomes too different from the data the model has been trained on. In this case, the model is likely to become obsolete and needs to be trained again.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1309708688b86629785c2dd91344748cb6679acc2904a366c0f52cc1186b769c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
